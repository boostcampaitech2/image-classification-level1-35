{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c7c18c",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce0d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from model import MyModel\n",
    "from tqdm import tqdm\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c593b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd0357",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2889041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, num_classes: int = 1000):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#         )\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(32, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cae97",
   "metadata": {},
   "source": [
    "## 2. Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f19340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))['image']\n",
    "        return image.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba44e5dd",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91f50190-9946-4874-b974-df69a04bf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'efficientnet_b3a_aug_chpolicy_trfm_probup'\n",
    "models = [\n",
    " f'../models/{model_name}/fold_1_{model_name}_best.pt',\n",
    "   f'../models/{model_name}/fold_2_{model_name}_best.pt',\n",
    "    f'../models/{model_name}/fold_3_{model_name}_best.pt',\n",
    "    f'../models/{model_name}/fold_4_{model_name}_best.pt',\n",
    "    f'../models/{model_name}/fold_5_{model_name}_best.pt'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "def9e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference ../models/efficientnet_b3a_aug_chpolicy_trfm_probup/fold_1_efficientnet_b3a_aug_chpolicy_trfm_probup_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [05:46<00:00, 36.35it/s]\n",
      "  0%|          | 0/12600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference ../models/efficientnet_b3a_aug_chpolicy_trfm_probup/fold_2_efficientnet_b3a_aug_chpolicy_trfm_probup_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [04:38<00:00, 45.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference ../models/efficientnet_b3a_aug_chpolicy_trfm_probup/fold_3_efficientnet_b3a_aug_chpolicy_trfm_probup_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [05:37<00:00, 37.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference ../models/efficientnet_b3a_aug_chpolicy_trfm_probup/fold_4_efficientnet_b3a_aug_chpolicy_trfm_probup_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [05:50<00:00, 35.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference ../models/efficientnet_b3a_aug_chpolicy_trfm_probup/fold_5_efficientnet_b3a_aug_chpolicy_trfm_probup_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [05:46<00:00, 36.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 12600, 18)\n",
      "(12600,)\n",
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform_valid = Compose([\n",
    "        CenterCrop(height=384, width=384, p=1.0),\n",
    "        #RandomCrop(always_apply=True, height=384, width=384, p=1.0),\n",
    "        Normalize(mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246), max_pixel_value=255.0, p=1.0),\n",
    "        ToTensorV2(p=1.0),\n",
    "        #T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform_valid)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "results = np.array([0]*18)\n",
    "all_predictions = []\n",
    "for model_path in models:\n",
    "    print(f'Inference {model_path}')\n",
    "    model = torch.load(model_path).to(device)\n",
    "\n",
    "    #MyModel(num_classes=18).to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "    all_prediction = []\n",
    "    for images in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            pred = model(images)\n",
    "            #pred = pred.argmax(dim=-1)\n",
    "            all_prediction.extend(pred.cpu().numpy())\n",
    "    all_predictions.append(all_prediction)\n",
    "\n",
    "print(np.array(all_predictions).shape)\n",
    "np.save(f'../results/fold_{model_name}_predict_result_center_crop.npy', np.array(all_predictions))\n",
    "all_predictions = np.array(all_predictions)\n",
    "master_predictions = all_predictions[0] + all_predictions[1] + all_predictions[2] + all_predictions[3] + all_predictions[4]\n",
    "ensemble_result = np.argmax(master_predictions, axis=1)\n",
    "print(ensemble_result.shape)\n",
    "\n",
    "submission['ans'] = ensemble_result\n",
    "submission.to_csv(f'../results/submision_fold_{model_name}_center_crop.csv', index=False)\n",
    "print('test inference is done!')\n",
    "\n",
    "# # print(results[:5])\n",
    "# # print(np.argmax(results, dim=1)[:5])\n",
    "# # submission['ans'] = np.argmax(results, dim=1)\n",
    "\n",
    "# # #제출할 파일을 저장합니다.\n",
    "# # submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "# # print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78494af5-eb9b-494e-a0f7-df69576748b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  1 13 ...  9  1  7]\n"
     ]
    }
   ],
   "source": [
    "print(ensemble_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6136d2a7-89dd-4364-8f0d-ac5fd669b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13  1 13 ...  9  1  7]\n"
     ]
    }
   ],
   "source": [
    "print(ensemble_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "54abf6dc-fbbb-4e62-bf29-30c6091f8828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     re = np.unique(np.argmax(all_predictions[i],axis=1), return_counts=True)\n",
    "# #     print(i)\n",
    "# #     for j in range(len(re[0])):\n",
    "# #         print(re[0][j], \":\", re[1][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a52a536-2eb8-42e0-ab33-a43fee9daab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "submission['ans'] = np.argmax(all_predictions[idx],axis=1)\n",
    "submission.to_csv(os.path.join(test_dir, f'submission_{idx}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4465e8-22df-42b2-a9fc-125c96071828",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.AdamP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94285722-a55b-4d18-ad7e-0ef62b2950fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[13  1 13 ...  9  1  7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84cf6891-84a1-4ce0-bd66-875a85d90bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73130ade-963e-453c-942f-dc76d0c53ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12600/12600 [06:14<00:00, 33.61it/s]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('../models/efficientnet_b3a_aug_plus_ch_trfm_normalize/fold_1_efficientnet_b3a_aug_plus_ch_trfm_normalize_best.pt').to(device)\n",
    "#MyModel(num_classes=18).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_prediction = []\n",
    "for images in tqdm(loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_prediction.extend(pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e67a40-a582-47f1-bf7e-936a111ffb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['ans'] = all_prediction\n",
    "submission.to_csv(os.path.join(test_dir, './submission_non_ensemble.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f7c7d98-0c16-48a1-829b-814cdd157420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39001ede-6802-4d65-8e0c-0af840a2ae3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_prediction, return_counter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2854723-74e8-4fdc-af84-b6c214622e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(f'../results/submision_fold_{model_name}_center_crop.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6437057-13e2-46c4-acf9-b307671869be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea4618-0890-42a2-8dac-b6bde113621a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2cc98-70c9-4fc8-8137-c5fecf413fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe4916-7da9-4cde-ac2b-6d8899677897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}